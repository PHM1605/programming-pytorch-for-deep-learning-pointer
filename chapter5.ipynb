{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b1bf64-87cb-4915-b4ce-075110e0b23a",
   "metadata": {},
   "source": [
    "# Text classification\n",
    "## Recurrent Neural Networks\n",
    "## LSTM\n",
    "## Gated Recurrent Units\n",
    "## biLSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480c8e1b-624d-4560-a9e6-66fdd20402d7",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d2f9ab06-c283-49a6-8435-d3521738cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8c6a1dc8-ee82-469b-b237-64225580c6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0719, -1.2779]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_mat_embed = nn.Embedding(5,2)\n",
    "cat_tensor = torch.LongTensor([1]) # position of word \"cat\" in dictionary\n",
    "cat_mat_embed.forward(cat_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f50a433-b774-4a23-b5f4-320a4bc04d5c",
   "metadata": {},
   "source": [
    "## Tweets Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6d4db25f-42f0-4763-9d51-36476946cfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5a321e62-84de-487a-8e24-91a9f319b718",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsDF = pd.read_csv(\"tweet-data/training.1600000.processed.noemoticon.csv\", engine=\"python\", encoding=\"latin-1\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a3ee7186-6743-4e03-acf4-07e1f51a3f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0           1                             2         3                4  \\\n",
       "0  0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                                   5  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsDF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dd38f6c4-54ad-4083-b1e8-25682aaa87df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "0    800000\n",
       "4    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsDF[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8be0696c-c773-4d48-8f7a-bc3d28148266",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsDF[\"sentiment_cat\"] = tweetsDF[0].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "460033ef-6322-4c6c-9a20-fd2bbba76b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsDF[\"sentiment\"] = tweetsDF[\"sentiment_cat\"].cat.codes # to make class e.g. (0,4,5) to (0,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "96484c0f-ced6-48aa-ab15-1ae9d74f02ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsDF.to_csv(\"tweet-data/train-processed.csv\", header=None, index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ceb4e14a-e757-46c8-bc1d-1382a7212ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsDF.sample(10000).to_csv(\"tweet-data/train-processed-sample.csv\", header=None, index=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72479846-7fc3-49ac-90a4-bb12dd41cbd4",
   "metadata": {},
   "source": [
    "### torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ed0efc26-0367-4d3b-8204-115cd6951f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "COLS = [\"score\", \"id\", \"date\", \"query\", \"name\", \"tweet\", \"category\", \"label\"]\n",
    "df = pd.read_csv(\"tweet-data/train-processed.csv\", header=None, names=COLS, encoding_errors=\"ignore\")\n",
    "df = df[[\"tweet\", \"label\"]].dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "38a098fa-5582-4006-9a75-7d32ba7c2861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PHM16\\anaconda3\\envs\\python310\\lib\\site-packages\\torchtext\\data\\utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1280000, 160000, 160000)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = get_tokenizer('spacy')\n",
    "\n",
    "class TweetDS(Dataset):\n",
    "    def __init__(self, frame):\n",
    "        self.frame = frame.reset_index(drop=True)\n",
    "    def __len__(self):\n",
    "        return len(self.frame)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.frame.iloc[idx]\n",
    "        return str(row[\"tweet\"]), int(row[\"label\"])\n",
    "        \n",
    "full_ds = TweetDS(df)\n",
    "N = len(full_ds)\n",
    "n_train = int(0.8 * N)\n",
    "n_valid = int(0.1 * N)\n",
    "n_test = N - n_train - n_valid\n",
    "train_ds, valid_ds, test_ds = random_split(full_ds, [n_train, n_valid, n_test])\n",
    "len(train_ds), len(valid_ds), len(test_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a2167905-7828-4ab7-8dcc-f29620681460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tok(\"Hello World!\") => [\"hello\", \"world\", \"!\"]\n",
    "def tok(text):\n",
    "    return [t.lower() for t in tokenizer(text)]\n",
    "def yield_tokens(ds):\n",
    "    for text, label in ds:\n",
    "        yield tok(text)\n",
    "# vocab: {'<unk>':0, '<pad>':1, 'hello':2, 'world':3, '!':4, 'i':5, 'am':6, 'hungry':7, ...}\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_ds), specials=[\"<unk>\", \"<pad>\"], max_tokens=20002)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1d5dd847-5326-4311-ba9d-659708ec2efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_pipeline(\"Hello World!\") => [2,3,5] (index in dict)\n",
    "def text_pipeline(x: str):\n",
    "    return [vocab[token] for token in tok(x)]\n",
    "\n",
    "# batch: [ (\"Hello World!\", 1), (\"I am hungry\", 0) ]\n",
    "def collate(batch):\n",
    "    xs, ys = zip(*batch) # xs: (\"Hello World!\", \"I am hungry\"); ys: (1,0); \n",
    "    xs = [torch.tensor(text_pipeline(x), dtype=torch.long) for x in xs] # [(2,4), (3,5,6)], \n",
    "    xpad = nn.utils.rnn.pad_sequence(xs, batch_first=True, padding_value=vocab[\"<pad>\"]) # [[2,4,1],[3,5,6]] with 1=<pad>\n",
    "    y = torch.tensor(ys, dtype=torch.long).view(-1)\n",
    "    return xpad.to(device), y.to(device)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=32, shuffle=False, collate_fn=collate)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1748a342-5774-4139-a724-1d0bb5ff5ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 20002\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocab size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8d488261-4160-4808-b97e-90cb25963e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 797378), ('!', 723441), ('.', 646538), (' ', 469854), ('to', 452934), ('the', 417901), (',', 386468), ('a', 304651), ('my', 252921), ('it', 242787)]\n"
     ]
    }
   ],
   "source": [
    "# Most common words\n",
    "from collections import Counter \n",
    "counter = Counter()\n",
    "for text, _ in train_ds:\n",
    "    counter.update(tok(text))\n",
    "print(counter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abcd4b1-c33c-498a-8f55-7da36511694b",
   "metadata": {},
   "source": [
    "## Creating our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "02794477-91e4-4869-844e-1b132b55bc18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OurFirstLSTM(\n",
       "  (embedding): Embedding(20002, 300)\n",
       "  (encoder): LSTM(300, 100, batch_first=True)\n",
       "  (predictor): Linear(in_features=100, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classifying tweets\n",
    "class OurFirstLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim) # [seq_len, vocab_size] => [seq_len, embedding_dim]\n",
    "        self.encoder = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        self.predictor = nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        output, (hidden, _) = self.encoder(self.embedding(seq)) # hidden: [1,batch,hidden]\n",
    "        preds = self.predictor(hidden.squeeze(0)) # [hidden,] => [batch,2]\n",
    "        return preds\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = OurFirstLSTM(100, 300, 20002)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab16245-5186-4206-9284-054b74547d82",
   "metadata": {},
   "source": [
    "## Notice\n",
    "### Binary: BCEWithLogits => our Model do NOT have sigmoid (it's inside Loss Function already)\n",
    "### Binary: BCELoss => our Model DO HAVE sigmoid\n",
    "### Multiclass: CrossEntropyLoss => our Model do NOT have log-softmax (it's inside the Loss Function already)\n",
    "### Multiclass: NLLLoss => our Model MUST HAVE log-softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2907cf3a-a6c8-4a47-9c24-53183bbb4870",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=2e-2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(epochs, model, optimizer, criterion, train_iterator, valid_iterator):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "        for batch_idx, (features, label) in enumerate(train_iterator):\n",
    "            optimizer.zero_grad()\n",
    "            predict = model(features)\n",
    "            loss = criterion(predict, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item() * label.size(0)\n",
    "        training_loss /= len(train_iterator.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (features, label) in enumerate(valid_iterator):\n",
    "                predict = model(features)\n",
    "                loss = criterion(predict, label)\n",
    "                valid_loss += loss.data.item() * features.size(0)\n",
    "        valid_loss /= len(valid_iterator.dataset)\n",
    "\n",
    "        print(\"Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}\".format(epoch, training_loss, valid_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "323a278b-db31-4ad1-b370-d5a22d55f5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_tweet(tweet):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ids = torch.tensor([text_pipeline(tweet)], dtype=torch.long, device=device)\n",
    "        logits = model(ids)\n",
    "        pred = int(torch.argmax(logits, dim=1).item())\n",
    "    categories = {0: \"Negative\", 1: \"Positive\"}\n",
    "    return categories[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5001dbd1-8b8e-4026-a052-b064aa373cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.66, Validation Loss: 0.65\n",
      "Epoch: 2, Training Loss: 0.66, Validation Loss: 0.65\n",
      "Epoch: 3, Training Loss: 0.66, Validation Loss: 0.67\n",
      "Epoch: 4, Training Loss: 0.67, Validation Loss: 0.65\n",
      "Epoch: 5, Training Loss: 0.67, Validation Loss: 0.66\n"
     ]
    }
   ],
   "source": [
    "train(5, model, optimizer, criterion, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6aecc0d5-d2bb-408b-9a16-f45bc87e8b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "print(classify_tweet(\"I love this!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fe8d97d0-1123-4bf0-9239-b5256d99bdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "print(classify_tweet(\"This is terrible\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ffbacf-ccb1-49e5-839c-9ce72c000781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
